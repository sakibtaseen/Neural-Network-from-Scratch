{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=15, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate(hidden_1=10, hidden_2=10, output=1):\n",
    "    \n",
    "    W1 = np.random.randn(hidden_1, X.shape[1]) \n",
    "    W2 = np.random.randn(hidden_2, hidden_1) \n",
    "    W3 = np.random.randn(output, hidden_2) \n",
    "    b1 = np.random.randn(hidden_1, 1) \n",
    "    b2 = np.random.randn(hidden_2, 1) \n",
    "    b3 = np.random.randn(output, 1)\n",
    "    \n",
    "    return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = lambda x: 1/(1 + np.exp(-x))\n",
    "dsigma = lambda x: sigma(x)*(1-sigma(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(W1, W2, W3, b1, b2, b3, x):\n",
    "    a1 = W1@x + b1\n",
    "    z1 = sigma(a1)\n",
    "    a2 = W2@z1 + b2\n",
    "    z2 = sigma(a2)\n",
    "    a3 = W3@z2 + b3\n",
    "    z3 = sigma(a3)\n",
    "    \n",
    "    return a1, a2, a3, z1, z2, z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_W3(a1, a2, a3, z1, z2, z3, X, y) :\n",
    "    d = (2 * (z3 - y))\n",
    "    d = d * dsigma(z3)\n",
    "    d = d @ z2.T \n",
    "    return d\n",
    "\n",
    "def d_W2(a1, a2, a3, z1, z2, z3, X, y):\n",
    "    d = (2 * (z3 - y))\n",
    "    d = d * dsigma(z3)\n",
    "    d = (d.T@W3).T\n",
    "    d = d * dsigma(z2)\n",
    "    d = d @ z1.T\n",
    "    return d\n",
    "\n",
    "def d_W1(a1, a2, a3, z1, z2, z3, X, y):\n",
    "    d = (2 * (z3 - y))\n",
    "    d = d * dsigma(z3)\n",
    "    d = (d.T@W3).T\n",
    "    d = d * dsigma(z2)\n",
    "    d = (d.T@W2).T\n",
    "    d = d * dsigma(z1)\n",
    "    d = d @ X.T\n",
    "    return d\n",
    "\n",
    "def d_b3 (a1, a2, a3, z1, z2, z3, X, y) :\n",
    "    d = (2 * (z3 - y))\n",
    "    d = d * dsigma(z3)\n",
    "    d = d*1 \n",
    "    return d\n",
    "\n",
    "def d_b2 (a1, a2, a3, z1, z2, z3, X, y) :\n",
    "    d = (2 * (z3 - y))\n",
    "    d = d * dsigma(z3)\n",
    "    d = (d.T@W3).T\n",
    "    d = d * dsigma(z2)\n",
    "    d = d*1\n",
    "    return d\n",
    "\n",
    "def d_b1 (a1, a2, a3, z1, z2, z3, X, y) :\n",
    "    d = (2 * (z3 - y)) \n",
    "    d = d * dsigma(z3)\n",
    "    d = (d.T@W3).T\n",
    "    d = d * dsigma(z2)\n",
    "    d = (d.T@W2).T\n",
    "    d = d * dsigma(z1)\n",
    "    d = d*1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W1, W2, W3, b1, b2, b3, dw1, dw2, dw3, db1, db2, db3, lr=0.0001):\n",
    "    W1 = W1 - lr*dw1\n",
    "    W2 = W2 - lr*dw2\n",
    "    W3 = W3 - lr*dw3\n",
    "    b1 = b1 - lr*db1\n",
    "    b2 = b2 - lr*db2\n",
    "    b3 = b3 - lr*db3\n",
    "    \n",
    "    return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 300\n",
    "W1, W2, W3, b1, b2, b3 = initiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, _, target_before = feed_forward(W1, W2, W3, b1, b2, b3, X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in 0 step is:0.18541427956040105\n",
      "Total loss in 1 step is:0.11707942092508819\n",
      "Total loss in 2 step is:0.1043612422884194\n",
      "Total loss in 3 step is:0.10067640642030869\n",
      "Total loss in 4 step is:0.09942661139799193\n",
      "Total loss in 5 step is:0.09878361554030723\n",
      "Total loss in 6 step is:0.09832546739204702\n",
      "Total loss in 7 step is:0.09794693634687753\n",
      "Total loss in 8 step is:0.09761977598850896\n",
      "Total loss in 9 step is:0.09733895662884583\n",
      "Total loss in 10 step is:0.09710721004498094\n",
      "Total loss in 11 step is:0.09692899246590929\n",
      "Total loss in 12 step is:0.09680807483354212\n",
      "Total loss in 13 step is:0.09674710859188192\n",
      "Total loss in 14 step is:0.09674774321707261\n",
      "Total loss in 15 step is:0.09681048729578708\n",
      "Total loss in 16 step is:0.09693417882303103\n",
      "Total loss in 17 step is:0.09711511073920752\n",
      "Total loss in 18 step is:0.09734614812990347\n",
      "Total loss in 19 step is:0.09761660975612721\n",
      "Total loss in 20 step is:0.09791345596774481\n",
      "Total loss in 21 step is:0.09822345031442964\n",
      "Total loss in 22 step is:0.09853539383651722\n",
      "Total loss in 23 step is:0.09884163500069994\n",
      "Total loss in 24 step is:0.09913844834378009\n",
      "Total loss in 25 step is:0.09942525807394637\n",
      "Total loss in 26 step is:0.09970317444958311\n",
      "Total loss in 27 step is:0.09997377890102312\n",
      "Total loss in 28 step is:0.10023890551405328\n",
      "Total loss in 29 step is:0.10050137855737455\n",
      "Total loss in 30 step is:0.10076579721027747\n",
      "Total loss in 31 step is:0.10103788445965359\n",
      "Total loss in 32 step is:0.1013215756345761\n",
      "Total loss in 33 step is:0.10161527280427671\n",
      "Total loss in 34 step is:0.10190982337294205\n",
      "Total loss in 35 step is:0.10218850141478081\n",
      "Total loss in 36 step is:0.1024277230902099\n",
      "Total loss in 37 step is:0.10259803084126165\n",
      "Total loss in 38 step is:0.10266588422003889\n",
      "Total loss in 39 step is:0.10259786828201511\n",
      "Total loss in 40 step is:0.10236807789539253\n",
      "Total loss in 41 step is:0.10196658958668862\n",
      "Total loss in 42 step is:0.10140384976790648\n",
      "Total loss in 43 step is:0.10070805679336445\n",
      "Total loss in 44 step is:0.09991922120955588\n",
      "Total loss in 45 step is:0.0990830181902146\n",
      "Total loss in 46 step is:0.0982442183162843\n",
      "Total loss in 47 step is:0.09744064726547848\n",
      "Total loss in 48 step is:0.09669937960830918\n",
      "Total loss in 49 step is:0.09603573923627849\n",
      "Total loss in 50 step is:0.09545487313585728\n",
      "Total loss in 51 step is:0.09495471848653865\n",
      "Total loss in 52 step is:0.0945282021052233\n",
      "Total loss in 53 step is:0.09416411827004681\n",
      "Total loss in 54 step is:0.09384854826331841\n",
      "Total loss in 55 step is:0.09356742855269135\n",
      "Total loss in 56 step is:0.09330838394098177\n",
      "Total loss in 57 step is:0.09306185507591787\n",
      "Total loss in 58 step is:0.09282354942598184\n",
      "Total loss in 59 step is:0.09259512832992632\n",
      "Total loss in 60 step is:0.0923805584865144\n",
      "Total loss in 61 step is:0.09218291147465094\n",
      "Total loss in 62 step is:0.09200439649924916\n",
      "Total loss in 63 step is:0.09184668732975694\n",
      "Total loss in 64 step is:0.09171038211496713\n",
      "Total loss in 65 step is:0.09159474313548154\n",
      "Total loss in 66 step is:0.09149801011684386\n",
      "Total loss in 67 step is:0.09141783891392867\n",
      "Total loss in 68 step is:0.0913519194721227\n",
      "Total loss in 69 step is:0.09129903267486\n",
      "Total loss in 70 step is:0.09126036288745763\n",
      "Total loss in 71 step is:0.09124022330410707\n",
      "Total loss in 72 step is:0.09124459196825399\n",
      "Total loss in 73 step is:0.09127758752433016\n",
      "Total loss in 74 step is:0.09133896041449945\n",
      "Total loss in 75 step is:0.09142436927175385\n",
      "Total loss in 76 step is:0.09152686641123413\n",
      "Total loss in 77 step is:0.09163805648633344\n",
      "Total loss in 78 step is:0.09174923285331388\n",
      "Total loss in 79 step is:0.09185268313236947\n",
      "Total loss in 80 step is:0.0919427110885974\n",
      "Total loss in 81 step is:0.09201598023435477\n",
      "Total loss in 82 step is:0.09207114909011915\n",
      "Total loss in 83 step is:0.09210843725439985\n",
      "Total loss in 84 step is:0.09212988738260068\n",
      "Total loss in 85 step is:0.09214018927566835\n",
      "Total loss in 86 step is:0.09214643684127546\n",
      "Total loss in 87 step is:0.09215507450504043\n",
      "Total loss in 88 step is:0.09216803971425168\n",
      "Total loss in 89 step is:0.09218242932917024\n",
      "Total loss in 90 step is:0.09219347583182932\n",
      "Total loss in 91 step is:0.09219739762214774\n",
      "Total loss in 92 step is:0.0921925759636511\n",
      "Total loss in 93 step is:0.09217943525141045\n",
      "Total loss in 94 step is:0.0921597173540922\n",
      "Total loss in 95 step is:0.09213565598886232\n",
      "Total loss in 96 step is:0.09210938026669976\n",
      "Total loss in 97 step is:0.09208266185978238\n",
      "Total loss in 98 step is:0.0920569088904062\n",
      "Total loss in 99 step is:0.0920332299831312\n",
      "Total loss in 100 step is:0.09201245484493738\n",
      "Total loss in 101 step is:0.09199510494481875\n",
      "Total loss in 102 step is:0.09198136508896913\n",
      "Total loss in 103 step is:0.09197109149630439\n",
      "Total loss in 104 step is:0.0919638500853198\n",
      "Total loss in 105 step is:0.09195896071378307\n",
      "Total loss in 106 step is:0.091955535183308\n",
      "Total loss in 107 step is:0.0919525161265677\n",
      "Total loss in 108 step is:0.09194873261620191\n",
      "Total loss in 109 step is:0.0919429817194338\n",
      "Total loss in 110 step is:0.0919341276453098\n",
      "Total loss in 111 step is:0.09192119311622682\n",
      "Total loss in 112 step is:0.09190341403022927\n",
      "Total loss in 113 step is:0.09188024236366513\n",
      "Total loss in 114 step is:0.0918513043684688\n",
      "Total loss in 115 step is:0.09181633728705492\n",
      "Total loss in 116 step is:0.09177513161280886\n",
      "Total loss in 117 step is:0.09172750053564747\n",
      "Total loss in 118 step is:0.09167328647416363\n",
      "Total loss in 119 step is:0.09161239562279942\n",
      "Total loss in 120 step is:0.09154483407580968\n",
      "Total loss in 121 step is:0.0914707259384402\n",
      "Total loss in 122 step is:0.09139032590739302\n",
      "Total loss in 123 step is:0.09130405503496282\n",
      "Total loss in 124 step is:0.09121256610100605\n",
      "Total loss in 125 step is:0.09111681536917544\n",
      "Total loss in 126 step is:0.09101810754296936\n",
      "Total loss in 127 step is:0.09091808546806593\n",
      "Total loss in 128 step is:0.09081864651031651\n",
      "Total loss in 129 step is:0.09072178477020247\n",
      "Total loss in 130 step is:0.09062938507940976\n",
      "Total loss in 131 step is:0.09054302223566608\n",
      "Total loss in 132 step is:0.09046382816889317\n",
      "Total loss in 133 step is:0.09039246839744546\n",
      "Total loss in 134 step is:0.09032922337689536\n",
      "Total loss in 135 step is:0.09027412182087288\n",
      "Total loss in 136 step is:0.09022704893869753\n",
      "Total loss in 137 step is:0.09018777204483563\n",
      "Total loss in 138 step is:0.09015588342811812\n",
      "Total loss in 139 step is:0.09013072147973844\n",
      "Total loss in 140 step is:0.09011135987088512\n",
      "Total loss in 141 step is:0.09009674437137445\n",
      "Total loss in 142 step is:0.09008601826921156\n",
      "Total loss in 143 step is:0.09007898207405614\n",
      "Total loss in 144 step is:0.09007644677750007\n",
      "Total loss in 145 step is:0.09008015180593969\n",
      "Total loss in 146 step is:0.09009225622504724\n",
      "Total loss in 147 step is:0.09011486403143441\n",
      "Total loss in 148 step is:0.09014990368916485\n",
      "Total loss in 149 step is:0.09019919980550638\n",
      "Total loss in 150 step is:0.09026439604569465\n",
      "Total loss in 151 step is:0.09034644999031789\n",
      "Total loss in 152 step is:0.09044461481507764\n",
      "Total loss in 153 step is:0.09055537396955117\n",
      "Total loss in 154 step is:0.09067229292064408\n",
      "Total loss in 155 step is:0.09078722294176063\n",
      "Total loss in 156 step is:0.09089228027380337\n",
      "Total loss in 157 step is:0.09098226358842987\n",
      "Total loss in 158 step is:0.09105769214386401\n",
      "Total loss in 159 step is:0.09112684649835029\n",
      "Total loss in 160 step is:0.09120267811675771\n",
      "Total loss in 161 step is:0.09129294550497713\n",
      "Total loss in 162 step is:0.09139204697890273\n",
      "Total loss in 163 step is:0.09148898845358315\n",
      "Total loss in 164 step is:0.09158099467245842\n",
      "Total loss in 165 step is:0.09166908523125039\n",
      "Total loss in 166 step is:0.09174882718416634\n",
      "Total loss in 167 step is:0.09181493417742163\n",
      "Total loss in 168 step is:0.09186850283077246\n",
      "Total loss in 169 step is:0.09191633311913139\n",
      "Total loss in 170 step is:0.09196696883030168\n",
      "Total loss in 171 step is:0.09202777319151258\n",
      "Total loss in 172 step is:0.09210335123359205\n",
      "Total loss in 173 step is:0.09219416525028205\n",
      "Total loss in 174 step is:0.09229560736630033\n",
      "Total loss in 175 step is:0.09239947043934134\n",
      "Total loss in 176 step is:0.09249788899395134\n",
      "Total loss in 177 step is:0.09258665411340188\n",
      "Total loss in 178 step is:0.09266543935182905\n",
      "Total loss in 179 step is:0.09273587367750416\n",
      "Total loss in 180 step is:0.09279991217437629\n",
      "Total loss in 181 step is:0.09285968803569387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in 182 step is:0.09291820744425888\n",
      "Total loss in 183 step is:0.09297966926002607\n",
      "Total loss in 184 step is:0.09304889983352864\n",
      "Total loss in 185 step is:0.09313028419394477\n",
      "Total loss in 186 step is:0.09322679301140076\n",
      "Total loss in 187 step is:0.09333943691828467\n",
      "Total loss in 188 step is:0.09346728316383654\n",
      "Total loss in 189 step is:0.09360808271735502\n",
      "Total loss in 190 step is:0.09375936130222112\n",
      "Total loss in 191 step is:0.09391956176878721\n",
      "Total loss in 192 step is:0.09408870098995749\n",
      "Total loss in 193 step is:0.0942681368211878\n",
      "Total loss in 194 step is:0.09445927012111854\n",
      "Total loss in 195 step is:0.09466151293864948\n",
      "Total loss in 196 step is:0.0948707670829014\n",
      "Total loss in 197 step is:0.09507921453629321\n",
      "Total loss in 198 step is:0.0952770229890815\n",
      "Total loss in 199 step is:0.09545893066012491\n",
      "Total loss in 200 step is:0.09562994358206756\n",
      "Total loss in 201 step is:0.0957984280360936\n",
      "Total loss in 202 step is:0.09597008920701862\n",
      "Total loss in 203 step is:0.09615029753373798\n",
      "Total loss in 204 step is:0.09634271072273003\n",
      "Total loss in 205 step is:0.09654683202617884\n",
      "Total loss in 206 step is:0.09675962992897591\n",
      "Total loss in 207 step is:0.09697794442310535\n",
      "Total loss in 208 step is:0.0971983641759819\n",
      "Total loss in 209 step is:0.09741524810707344\n",
      "Total loss in 210 step is:0.09762067711939401\n",
      "Total loss in 211 step is:0.097806352152853\n",
      "Total loss in 212 step is:0.09796427752260718\n",
      "Total loss in 213 step is:0.09808859975867312\n",
      "Total loss in 214 step is:0.09817817548932505\n",
      "Total loss in 215 step is:0.09823691657083208\n",
      "Total loss in 216 step is:0.09827312666754864\n",
      "Total loss in 217 step is:0.09829844739904908\n",
      "Total loss in 218 step is:0.09832554146485106\n",
      "Total loss in 219 step is:0.09836422842486865\n",
      "Total loss in 220 step is:0.09841712000037399\n",
      "Total loss in 221 step is:0.0984782817758521\n",
      "Total loss in 222 step is:0.09853795069213167\n",
      "Total loss in 223 step is:0.09858886111456984\n",
      "Total loss in 224 step is:0.09862807445552176\n",
      "Total loss in 225 step is:0.0986553040001032\n",
      "Total loss in 226 step is:0.09867101217310485\n",
      "Total loss in 227 step is:0.09867542236096168\n",
      "Total loss in 228 step is:0.09866832817221573\n",
      "Total loss in 229 step is:0.09864934116273184\n",
      "Total loss in 230 step is:0.0986183112656738\n",
      "Total loss in 231 step is:0.09857578024308773\n",
      "Total loss in 232 step is:0.09852325502941048\n",
      "Total loss in 233 step is:0.09846299348239526\n",
      "Total loss in 234 step is:0.09839726268999455\n",
      "Total loss in 235 step is:0.09832756145524599\n",
      "Total loss in 236 step is:0.09825441962080345\n",
      "Total loss in 237 step is:0.09817781395303794\n",
      "Total loss in 238 step is:0.09809771030170435\n",
      "Total loss in 239 step is:0.0980143435883323\n",
      "Total loss in 240 step is:0.09792821545219463\n",
      "Total loss in 241 step is:0.09783996529015812\n",
      "Total loss in 242 step is:0.09775024429713715\n",
      "Total loss in 243 step is:0.09765964435323499\n",
      "Total loss in 244 step is:0.09756868000919691\n",
      "Total loss in 245 step is:0.09747779990624272\n",
      "Total loss in 246 step is:0.09738740357683338\n",
      "Total loss in 247 step is:0.09729785181398203\n",
      "Total loss in 248 step is:0.0972094777349904\n",
      "Total loss in 249 step is:0.09712263660674263\n",
      "Total loss in 250 step is:0.09703794360023674\n",
      "Total loss in 251 step is:0.09695740722337641\n",
      "Total loss in 252 step is:0.09688797876833025\n",
      "Total loss in 253 step is:0.09683662940316187\n",
      "Total loss in 254 step is:0.0967917302847099\n",
      "Total loss in 255 step is:0.09674209324546885\n",
      "Total loss in 256 step is:0.09668693577500773\n",
      "Total loss in 257 step is:0.0966285477566764\n",
      "Total loss in 258 step is:0.09656908507695845\n",
      "Total loss in 259 step is:0.09651003127193264\n",
      "Total loss in 260 step is:0.0964522969566396\n",
      "Total loss in 261 step is:0.09639639301646345\n",
      "Total loss in 262 step is:0.09634258317761339\n",
      "Total loss in 263 step is:0.09629100166485668\n",
      "Total loss in 264 step is:0.09624173720855274\n",
      "Total loss in 265 step is:0.09619491366314693\n",
      "Total loss in 266 step is:0.09615078007780493\n",
      "Total loss in 267 step is:0.09610970941463087\n",
      "Total loss in 268 step is:0.09607203063152321\n",
      "Total loss in 269 step is:0.0960378815719726\n",
      "Total loss in 270 step is:0.09600723236372263\n",
      "Total loss in 271 step is:0.09597997398201355\n",
      "Total loss in 272 step is:0.09595594838646479\n",
      "Total loss in 273 step is:0.09593491133570617\n",
      "Total loss in 274 step is:0.09591650912687395\n",
      "Total loss in 275 step is:0.09590033015357842\n",
      "Total loss in 276 step is:0.0958859505220852\n",
      "Total loss in 277 step is:0.09587290068749178\n",
      "Total loss in 278 step is:0.09586066528960066\n",
      "Total loss in 279 step is:0.09584878238608555\n",
      "Total loss in 280 step is:0.09583694709868287\n",
      "Total loss in 281 step is:0.0958250494203631\n",
      "Total loss in 282 step is:0.09581317034900051\n",
      "Total loss in 283 step is:0.09580158745167958\n",
      "Total loss in 284 step is:0.09579080538495287\n",
      "Total loss in 285 step is:0.0957816258926246\n",
      "Total loss in 286 step is:0.09577542977469107\n",
      "Total loss in 287 step is:0.09577496512292882\n",
      "Total loss in 288 step is:0.09578464024283109\n",
      "Total loss in 289 step is:0.09580605678644218\n",
      "Total loss in 290 step is:0.09583041191013825\n",
      "Total loss in 291 step is:0.09584489965055093\n",
      "Total loss in 292 step is:0.09584590709032399\n",
      "Total loss in 293 step is:0.0958369206809664\n",
      "Total loss in 294 step is:0.0958219424871634\n",
      "Total loss in 295 step is:0.09580326192302772\n",
      "Total loss in 296 step is:0.09578156550345526\n",
      "Total loss in 297 step is:0.09575660913801091\n",
      "Total loss in 298 step is:0.09572797236985275\n",
      "Total loss in 299 step is:0.09569574121888157\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    loss = []\n",
    "    for index, x in enumerate(X):\n",
    "        x = np.reshape(x, (X.shape[1],1))\n",
    "        a1, a2, a3, z1, z2, z3 = feed_forward(W1, W2, W3, b1, b2, b3, x)\n",
    "        loss.append(np.square(z3 - y[index]))\n",
    "        dw1 = d_W1(a1, a2, a3, z1, z2, z3, x, y[index])\n",
    "        dw2 = d_W2(a1, a2, a3, z1, z2, z3, x, y[index])\n",
    "        dw3 = d_W3(a1, a2, a3, z1, z2, z3, x, y[index])\n",
    "        db1 = d_b1(a1, a2, a3, z1, z2, z3, x, y[index])\n",
    "        db2 = d_b2(a1, a2, a3, z1, z2, z3, x, y[index])\n",
    "        db3 = d_b3(a1, a2, a3, z1, z2, z3, x, y[index])\n",
    "        W1, W2, W3, b1, b2, b3 = update(W1, W2, W3, b1, b2, b3, dw1, dw2, dw3, db1, db2, db3, lr=0.05)\n",
    "    print(f'Total loss in {i} step is:{(np.sum(loss)/len(loss))}')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, _, target_after = feed_forward(W1, W2, W3, b1, b2, b3, X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_before = target_before.T.reshape(1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_after = target_after.T.reshape(1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'true':y, 'target_before':target_before, 'target_after':target_after}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(x):\n",
    "    if x>0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_before'] = df['target_before'].apply(mapping)\n",
    "df['label_after'] = df['target_after'].apply(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>target_before</th>\n",
       "      <th>target_after</th>\n",
       "      <th>label_before</th>\n",
       "      <th>label_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.193367</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.241660</td>\n",
       "      <td>0.857980</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.141380</td>\n",
       "      <td>0.054791</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.209010</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.348820</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.181895</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.334466</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.300777</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.376505</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.244624</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.340022</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.222047</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.229261</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0.207642</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0.279081</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0.198321</td>\n",
       "      <td>0.579503</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.233400</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0.261656</td>\n",
       "      <td>0.134846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.262875</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.163134</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.226385</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.205016</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.153575</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0.164005</td>\n",
       "      <td>0.248219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.344153</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233679</td>\n",
       "      <td>0.101580</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.146108</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.189581</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.180496</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0.202405</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>1</td>\n",
       "      <td>0.262935</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>1</td>\n",
       "      <td>0.235129</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>0</td>\n",
       "      <td>0.224155</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0</td>\n",
       "      <td>0.144753</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>1</td>\n",
       "      <td>0.337989</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>1</td>\n",
       "      <td>0.150189</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0</td>\n",
       "      <td>0.323280</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0</td>\n",
       "      <td>0.148201</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0</td>\n",
       "      <td>0.250126</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>1</td>\n",
       "      <td>0.227291</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0</td>\n",
       "      <td>0.291791</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>1</td>\n",
       "      <td>0.229706</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>0</td>\n",
       "      <td>0.124363</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>1</td>\n",
       "      <td>0.190259</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>1</td>\n",
       "      <td>0.125644</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>1</td>\n",
       "      <td>0.129706</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>1</td>\n",
       "      <td>0.253967</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1</td>\n",
       "      <td>0.143597</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0</td>\n",
       "      <td>0.129546</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>0</td>\n",
       "      <td>0.285227</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>1</td>\n",
       "      <td>0.159283</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>0</td>\n",
       "      <td>0.275891</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1</td>\n",
       "      <td>0.121784</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>1</td>\n",
       "      <td>0.229803</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0</td>\n",
       "      <td>0.222305</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0</td>\n",
       "      <td>0.302713</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0</td>\n",
       "      <td>0.220530</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1</td>\n",
       "      <td>0.229441</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0</td>\n",
       "      <td>0.233019</td>\n",
       "      <td>0.050852</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0</td>\n",
       "      <td>0.175093</td>\n",
       "      <td>0.857996</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     true  target_before  target_after  label_before  label_after\n",
       "0       0       0.193367      0.050852             0            0\n",
       "1       0       0.241660      0.857980             0            1\n",
       "2       0       0.141380      0.054791             0            0\n",
       "3       0       0.209010      0.050852             0            0\n",
       "4       1       0.348820      0.857996             0            1\n",
       "5       1       0.181895      0.857996             0            1\n",
       "6       0       0.334466      0.050852             0            0\n",
       "7       1       0.300777      0.857996             0            1\n",
       "8       0       0.376505      0.050852             0            0\n",
       "9       0       0.244624      0.050852             0            0\n",
       "10      1       0.340022      0.857996             0            1\n",
       "11      1       0.222047      0.857996             0            1\n",
       "12      1       0.229261      0.857996             0            1\n",
       "13      0       0.207642      0.050852             0            0\n",
       "14      0       0.279081      0.857996             0            1\n",
       "15      0       0.198321      0.579503             0            1\n",
       "16      1       0.233400      0.857996             0            1\n",
       "17      0       0.261656      0.134846             0            0\n",
       "18      1       0.262875      0.857996             0            1\n",
       "19      1       0.163134      0.857996             0            1\n",
       "20      1       0.226385      0.857996             0            1\n",
       "21      1       0.205016      0.050852             0            0\n",
       "22      1       0.153575      0.857996             0            1\n",
       "23      0       0.164005      0.248219             0            0\n",
       "24      1       0.344153      0.857996             0            1\n",
       "25      0       0.233679      0.101580             0            0\n",
       "26      0       0.146108      0.857996             0            1\n",
       "27      1       0.189581      0.857996             0            1\n",
       "28      1       0.180496      0.857996             0            1\n",
       "29      0       0.202405      0.857996             0            1\n",
       "..    ...            ...           ...           ...          ...\n",
       "970     1       0.262935      0.857996             0            1\n",
       "971     1       0.235129      0.857996             0            1\n",
       "972     0       0.224155      0.050852             0            0\n",
       "973     0       0.144753      0.050852             0            0\n",
       "974     1       0.337989      0.857996             0            1\n",
       "975     1       0.150189      0.857996             0            1\n",
       "976     0       0.323280      0.050852             0            0\n",
       "977     0       0.148201      0.050852             0            0\n",
       "978     0       0.250126      0.050852             0            0\n",
       "979     1       0.227291      0.857996             0            1\n",
       "980     0       0.291791      0.050852             0            0\n",
       "981     1       0.229706      0.857996             0            1\n",
       "982     0       0.124363      0.050852             0            0\n",
       "983     1       0.190259      0.857996             0            1\n",
       "984     1       0.125644      0.857996             0            1\n",
       "985     1       0.129706      0.857996             0            1\n",
       "986     1       0.253967      0.857996             0            1\n",
       "987     1       0.143597      0.857996             0            1\n",
       "988     0       0.129546      0.050852             0            0\n",
       "989     0       0.285227      0.050852             0            0\n",
       "990     1       0.159283      0.857996             0            1\n",
       "991     0       0.275891      0.050852             0            0\n",
       "992     1       0.121784      0.857996             0            1\n",
       "993     1       0.229803      0.050852             0            0\n",
       "994     0       0.222305      0.050852             0            0\n",
       "995     0       0.302713      0.050852             0            0\n",
       "996     0       0.220530      0.050852             0            0\n",
       "997     1       0.229441      0.857996             0            1\n",
       "998     0       0.233019      0.050852             0            0\n",
       "999     0       0.175093      0.857996             0            1\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[499   0]\n",
      " [501   0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(df['true'], df['label_before']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[442  57]\n",
      " [ 59 442]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(df['true'], df['label_after']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88       499\n",
      "           1       0.89      0.88      0.88       501\n",
      "\n",
      "    accuracy                           0.88      1000\n",
      "   macro avg       0.88      0.88      0.88      1000\n",
      "weighted avg       0.88      0.88      0.88      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df['true'], df['label_after']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(df['true'], df['label_before']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.884\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(df['true'], df['label_after']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
